"use strict";(self.webpackChunkportfolio_portal=self.webpackChunkportfolio_portal||[]).push([[109],{6026:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"spark-cluster-configuration","metadata":{"permalink":"/portfolio-portal/blog/spark-cluster-configuration","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2025-01-08-spark-cluster.md","source":"@site/blog/2025-01-08-spark-cluster.md","title":"Databricks Cluster Optimization","description":"Discussion Topic","date":"2025-01-08T00:00:00.000Z","tags":[{"inline":false,"label":"Spark","permalink":"/portfolio-portal/blog/tags/spark","description":"Spark discussion"}],"readingTime":3.2,"hasTruncateMarker":true,"authors":[{"name":"Lewis Quoc Quang","title":"Solution Architect @ Rackspace","url":"https://www.linkedin.com/in/trinh-quoc-quang/","page":{"permalink":"/portfolio-portal/blog/authors/lewis"},"socials":{"github":"https://github.com/QuangTrinh1612","linkedin":"https://www.linkedin.com/in/trinh-quoc-quang/"},"imageURL":"https://avatars.githubusercontent.com/u/55908196?s=400&u=401c81ca1269100ff4dcf3ddcf11a1ff035cab42&v=4","key":"lewis"}],"frontMatter":{"slug":"spark-cluster-configuration","title":"Databricks Cluster Optimization","authors":["lewis"],"tags":["Spark"]},"unlisted":false,"nextItem":{"title":"Spark Optimization - Reducing Shuffle","permalink":"/portfolio-portal/blog/spark-shuffle"}},"content":"## Discussion Topic\\r\\nWhat kind of cluster configuration should you use for your Databricks workloads?\\r\\n\\r\\n## Driver selection:\\r\\n- In most cases the driver is not required to be as large as the workers as the driver is mainly responsible for assigning tasks to the workers and does not perform any operations itself (except for single node cluster wherein the driver is the only node).\\r\\n\\r\\n\x3c!-- truncate --\x3e\\r\\n\\r\\n- Cases when a large driver is required:\\r\\n    - `Collect()`: When using `collect()` or any other such operation multiple times wherein the value is returned to the driver for usage as a variable. This puts a load on the driver as it collects the value as well as distributes it to the workers as a variable for usage down the line.\\r\\n    - Many small files problem: When reading from a file store and there are many small files that need to be read this puts a strain on the driver as it is responsible for assigning the files to be read to the respective workers. Tip: If the driver cannot be upsized, look into resizing the files in the file store to make the reads more performant.\\r\\n\\r\\n## Types of clusters, description and best practices:\\r\\n### Memory optimized cluster: \\r\\n\\r\\nFewer cores per worker i.e., less parallelism however this results in greater amount of memory per core. \\r\\n\\r\\n#### When to use\\r\\nJobs where shuffling/wide transformations will be required i.e., join, group by. Due to higher memory per core as well as fewer cores, there will be less shuffling and risk of spill onto disk due to increased data size is reduced. Useful for scenarios wherein there will be data caching required or if the same data is called on repeatedly either for analysis or a job.\\r\\n\\r\\n#### When not to use\\r\\nJobs where shuffling/wide transformations are not required, and the same task can be performed on smaller partitions in parallel unlike join/group by which requires looking up entire dataset for keys.\\r\\n\\r\\n### Compute optimized cluster: \\r\\n\\r\\nHigher number of cores per worker resulting in more parallel tasks being carried out, resulting in less memory per worker as compared to the above 2. \\r\\n\\r\\n#### When to use\\r\\nJobs carrying out the same task repeatedly ideally without wide transformations requiring shuffling.\\r\\n\\r\\n#### When not to use\\r\\nJobs where shuffling/wide transformations are present as it will result in a lot of shuffling and the result may be larger than the memory of the core causing spill onto disk.\\r\\n\\r\\nFor optimal performance tune spark configurations for spark.sql.files.maxPartitionBytes and spark.sql.shuffle.partitions according to the memory per core and cores per worker respectively.\\r\\n\\r\\n### General purpose cluster:\\r\\n\\r\\nComparable to compute optimized cluster having a larger amount of memory per worker and hence per core as compared to a compute optimized cluster. \\r\\n\\r\\n#### When to use\\r\\nRepetitive tasks being carried out on data where there might be some shuffling involved but not an extensive amount. Spilling to disk due to joins is less of a risk here due to higher memory per core. \\r\\n\\r\\n#### When not to use\\r\\nRepetitive tasks being carried out on data where no shuffling is involved (this will be more performant on compute optimized) or if there is are a lot of wide transformations required (this will be more performant on memory optimized). \\r\\n\\r\\n### Storage optimized cluster:\\r\\n\\r\\nEqual to memory optimized cluster with respect to memory and number of cores per worker difference being it has an increased disk throughput and IO.\\r\\n\\r\\n#### When to use\\r\\nSimilar use cases as memory optimized cluster, however especially recommended if the same data is to be read multiple times and there is risk of spill to disk due to increased data size after wide transformations.\\r\\n\\r\\n#### When not to use\\r\\nOperations that can be performed in parallel and do not require shuffling. Data is not required to be re-reads multiple times."},{"id":"spark-shuffle","metadata":{"permalink":"/portfolio-portal/blog/spark-shuffle","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2025-01-07-spark-shuffle/index.md","source":"@site/blog/2025-01-07-spark-shuffle/index.md","title":"Spark Optimization - Reducing Shuffle","description":"\u201cShuffling is the only thing which Nature cannot undo.\u201d \u2014 Arthur Eddington","date":"2025-01-07T00:00:00.000Z","tags":[{"inline":false,"label":"Spark","permalink":"/portfolio-portal/blog/tags/spark","description":"Spark discussion"}],"readingTime":6.885,"hasTruncateMarker":true,"authors":[{"name":"Lewis Quoc Quang","title":"Solution Architect @ Rackspace","url":"https://www.linkedin.com/in/trinh-quoc-quang/","page":{"permalink":"/portfolio-portal/blog/authors/lewis"},"socials":{"github":"https://github.com/QuangTrinh1612","linkedin":"https://www.linkedin.com/in/trinh-quoc-quang/"},"imageURL":"https://avatars.githubusercontent.com/u/55908196?s=400&u=401c81ca1269100ff4dcf3ddcf11a1ff035cab42&v=4","key":"lewis"}],"frontMatter":{"slug":"spark-shuffle","title":"Spark Optimization - Reducing Shuffle","authors":["lewis"],"tags":["Spark"]},"unlisted":false,"prevItem":{"title":"Databricks Cluster Optimization","permalink":"/portfolio-portal/blog/spark-cluster-configuration"},"nextItem":{"title":"Power BI Data Model - Best Practice Analyzer (BPA)","permalink":"/portfolio-portal/blog/pbi-bpa-rules"}},"content":"> \u201cShuffling is the only thing which Nature cannot undo.\u201d \u2014 **Arthur Eddington**\\r\\n\\r\\nI used to see people playing cards and using the word \u201cShuffle\u201d even before I knew how to play it. Shuffling in cards, play a very critical role to distribute \u201cpower\u201d, adding weightage to a player\u2019s hand. It is nothing but adding the _randomness_ in selection. When we want to distribute the cards for various games for example [_contract bridge_](https://en.wikipedia.org/wiki/Contract_bridge) shuffle is the way to create even/uneven distribution to 4 hands.\\r\\n\x3c!-- truncate --\x3e\\r\\n\\r\\n![Sweet hand](https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Bridge-Gro%C3%9Fschlemm.JPG/2560px-Bridge-Gro%C3%9Fschlemm.JPG)\\r\\n\\r\\n## Well, enough of playing cards!\\r\\n\\r\\nLet us understand how shuffle impacts big data computation. Ah, yes I think again I will use card shuffle to explain you. \ud83d\ude00\\r\\n\\r\\n![Chaos! I love that!](https://www.casino.org/blog/wp-content/uploads/shutterstock_417009952-875x583.jpg)\\r\\n\\r\\nLook at the above image and give me the answers of the below questions.\\r\\n\\r\\n*   How many black cards are present? \u2660\ufe0f\u2663\ufe0f\\r\\n*   How many of the red cards have numbers greater than 4? \u2665\ufe0f\u2666\ufe0f\\r\\n*   How many high value cards(showing off my knowledge eh!) are left in clubs? \u2663\ufe0f\\r\\n\\r\\nNo need to explain that you will tell me, \u201cYes, I can give you answers but let me arrange them first.\u201d Then you will do what is shown here.\\r\\n\\r\\n## The Shuffle in Big Data World\\r\\n\\r\\nTo answer my questions you must do the arrangement to order cards of same packs together like the above image. That means you need to find all cards of same family one by one and them order then A to K or vice versa. This operation of moving cards(data) to seek and order is actually called _Shuffle_ in big data world.\\r\\n\\r\\nImagine a situation when you are processing 1000s of GBs of data joining with similar magnitude and answering similar questions of different grains and groups. Yes, in distributed computing world exchanging data across machines, across networks creates so much exchange(I/O) that it slows down the computing process. Shuffle alone cause multiple stages in a big data job and delays the outcome.\\r\\n\\r\\n### How does shuffle work in Spark?\\r\\n\\r\\n![Spark Shuffle](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*HZelhB9lKu5NjdOwivQDjQ.png)\\r\\n\\r\\nIn Apache Spark, **_Shuffle_** describes the procedure in between reduce task and map task. Shuffling refers to the shuffle of data given. This operation is considered the costliest .The [shuffle operation](https://stackoverflow.com/questions/31386590/when-does-shuffling-occur-in-apache-spark) is implemented differently in Spark compared to Hadoop.\\r\\n\\r\\nOn the **map side**, each map task in Spark writes out a shuffle file (OS disk buffer) for every reducer \u2014 which corresponds to a logical block in Spark. These files are not intermediary in the sense that Spark does not merge them into larger partitioned ones. Since scheduling overhead in Spark is lesser, the number of mappers (`M`) and reducers(`R`) is far higher than in Hadoop. Thus, shipping `M*R` files to the respective reducers could result in significant overheads.\\r\\n\\r\\nSimilar to Hadoop, Spark also provide a parameter `spark.shuffle.compress` to specify compression libraries to compress map outputs. In this case, it could be `Snappy` (by default) or `LZF`. `Snappy` uses only 33KB of buffer for each opened file and significantly reduces risk of encountering out-of-memory errors.\\r\\n\\r\\nOn the **reduce side**, Spark requires all shuffled data to fit into memory of the corresponding reducer task. This would of course happen only in cases where the reducer task demands all shuffled data for a `GroupByKey` or a `ReduceByKey` operation, for instance. Spark throws an out-of-memory exception in this case, which has been quite a challenge because when spark spills over to disk it creates more problem of I/O and read slowness.\\r\\n\\r\\nAlso with Spark there is no overlapping copy phase, unlike Hadoop that has an overlapping copy phase where mappers push data to the reducers even before map is complete. This means that the shuffle is a _pull_ operation in Spark, compared to a _push_ operation in Hadoop. Each reducer should also maintain a network buffer to fetch map outputs. Size of this buffer is specified through the parameter `spark.reducer.maxMbInFlight` (by default, it is 48MB).\\r\\n\\r\\n## Tuning Spark to reduce shuffle\\r\\n\\r\\n### spark.sql.shuffle.partitions\\r\\n\\r\\nThe Spark SQL shuffle is a mechanism for redistributing or re-partitioning data so that the data is grouped differently across partitions. It is typically based on the volume of data you might have to reduce or increase the number of partitions of RDD/DataFrame using `spark.sql.shuffle.partitions` configuration or through code.\\r\\n\\r\\nUsing this configuration we can control the number of partitions of shuffle operations. By default, its value is `200`. But, 200 partitions does not make any sense if we have files of few GB(s). So, we should change them according to the amount of data we need to process via Spark SQL.\\r\\n\\r\\nLet\u2019s see a practical difference. Here I am creating a small two small dataframes with the most popular employee, department with two employees Daniel and me.\\r\\n\\r\\nThe default value of `spark.sql.shuffle.partitions` is 200. Let us run with default and see how much time it takes.\\r\\n\\r\\n> Time taken : **6060 ms** with spark.sql.shuffle.partitions = 200\\r\\n\\r\\nNow, if we do some modification with the config as we don\u2019t need 200 shuffle partitions for this such small amount of data if can be done faster. Here I am setting it to 2.\\r\\n\\r\\n> Time taken : **398 ms** with spark.sql.shuffle.partitions = 2\\r\\n\\r\\nSo, you can see tweaking the shuffle partition alone made it 15 times faster.\\r\\n\\r\\n## Reduce dataSet size\\r\\n\\r\\nThe classic rule of ETL. Filter as much as data near to the source is much important in spark as well. If you are dealing with lot of data, which has very fine grained aggregates and joins, it is pretty obvious there would be shuffles. It is always essential to control number of records before you start joins/aggregates so that data volume gets reduced by some %. Use appropriate filter predicates in your SQL query so Spark can push them down to the underlying datasource. Selective predicates are good. Use them as appropriate. Use partition filters if they are applicable.\\r\\n\\r\\n## Broadcast Broadcast Broadcast\\r\\n\\r\\nWhen you join two datasets, one large and one small the best option in Spark is to perform a broadcast join (map-side join). With broadcast join, you can very effectively join a large table (fact) with relatively small tables (dimensions) by avoiding sending all data of the large table over the network.\\r\\n\\r\\nYou can use broadcast function to mark a dataset to be broadcasted when used in a join operator. It uses spark.sql.autoBroadcastJoinThreshold setting to control the size of a table that will be broadcast to all worker nodes when performing a join.\\r\\n\\r\\n```sql\\r\\nSELECT /*+ BROADCAST(B) */ * FROM TableA A INNER JOIN TableB B ON A.key = B.key;\\r\\n```\\r\\n\\r\\nThis technique will broadcast the entire table B to all the executors and will help spark to avoid shuffle. The joins will will be local to all executors and thus it won\u2019t be needed any data to come across machines and there won\u2019t be any shuffle.\\r\\n\\r\\n## More Shuffles vs Lesser Shuffles\\r\\n--------------------------------\\r\\n\\r\\nSome times we encounter situations where we are joining multiple datasets but based on different keys. For example, let\u2019s check the sqls below.\\r\\n\\r\\n```sql\\r\\nSELECT * FROM TableA A INNER JOIN TableB B ON A.key1 = B.key1;\\r\\nSELECT * FROM TableB B INNER JOIN TableC C ON B.key2 = C.key2;\\r\\n```\\r\\n\\r\\n\\r\\nIt is evident that if we consider that while we read A and B it may or may not be partitioned to support the second join that means if we try to execute the joins without any such optimisation technique it might cause more shuffles. Key1 and Key2 across executors will not be evenly distributed. So in such cases we prefer to do repartition B or C accordingly. Repartition can be done on a column with a number specified or we can just do it with a random number which is suitable and comparable with the number of executor and core combination.\\r\\n\\r\\n```sql\\r\\nSELECT /*+ REPARTITION(key2)*/ * FROM TableB B;\\r\\n```\\r\\n\\r\\nThere are many other techniques to overcome shuffles which you will come across as much you start dealing with production level problems. I think the above ones are definitely the most important to start with.\\r\\n\\r\\nFor any type of help regarding career counselling, resume building, discussing designs or know more about latest data engineering trends and technologies reach out to me at [_anigos_](https://www.linkedin.com/in/anigos/)_._\\r\\n\\r\\n**_P.S : I don\u2019t charge money_**"},{"id":"pbi-bpa-rules","metadata":{"permalink":"/portfolio-portal/blog/pbi-bpa-rules","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-12-29-pbi-rules.md","source":"@site/blog/2024-12-29-pbi-rules.md","title":"Power BI Data Model - Best Practice Analyzer (BPA)","description":"Knowing general best practices for the PBI data model optimization, such as avoiding bi-directional relationships, reducing the column cardinality, avoiding DirectQuery whenever possible, or removing Auto Date/Time hidden tables, still remains the key requirement! But, Tabular Editor may help you quickly and easily identify potential violations of these practices \u2014 based on the insight gained, you can then decide if you want to apply the recommended practice(s) or keep your original data modeling logic in place through Best Practice Analyzer (BPA) tools.","date":"2024-12-29T00:00:00.000Z","tags":[{"inline":false,"label":"Data Governanace","permalink":"/portfolio-portal/blog/tags/data-governence","description":"Data Governanace Disscussion"}],"readingTime":0.86,"hasTruncateMarker":true,"authors":[{"name":"Lewis Quoc Quang","title":"Solution Architect @ Rackspace","url":"https://www.linkedin.com/in/trinh-quoc-quang/","page":{"permalink":"/portfolio-portal/blog/authors/lewis"},"socials":{"github":"https://github.com/QuangTrinh1612","linkedin":"https://www.linkedin.com/in/trinh-quoc-quang/"},"imageURL":"https://avatars.githubusercontent.com/u/55908196?s=400&u=401c81ca1269100ff4dcf3ddcf11a1ff035cab42&v=4","key":"lewis"}],"frontMatter":{"slug":"pbi-bpa-rules","title":"Power BI Data Model - Best Practice Analyzer (BPA)","authors":["lewis"],"tags":["Data Governance"]},"unlisted":false,"prevItem":{"title":"Spark Optimization - Reducing Shuffle","permalink":"/portfolio-portal/blog/spark-shuffle"},"nextItem":{"title":"Multitable SCD2 Joins - How to Unify Historical Changes","permalink":"/portfolio-portal/blog/multitable-scd2-joins"}},"content":"Knowing general best practices for the PBI data model optimization, such as avoiding bi-directional relationships, reducing the column cardinality, avoiding DirectQuery whenever possible, or removing Auto Date/Time hidden tables, still remains the key requirement! But, Tabular Editor may help you quickly and easily identify potential violations of these practices \u2014 based on the insight gained, you can then decide if you want to apply the recommended practice(s) or keep your original data modeling logic in place through Best Practice Analyzer (BPA) tools.\\r\\n\\r\\n\x3c!-- truncate --\x3e\\r\\n\\r\\n*And the best thing is that you can run BPA rules on your .bim file prior to CICD deployment to make sure the deployment is validated.*\\r\\n\\r\\nFor example:\\r\\n- Do not use floating point data types\\r\\n- Don\u2019t summarize numeric columns\\r\\n\\r\\n# Working with BPA Rules\\r\\n## Load BPA Rules\\r\\nWe can download the list of default BPA rules from this github (link) then import directly into tabular editor exe.\\r\\n1. To load the BPA rules, select the C# Script tab.\\r\\n2. Paste in the following script."},{"id":"multitable-scd2-joins","metadata":{"permalink":"/portfolio-portal/blog/multitable-scd2-joins","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-12-29-scd2.md","source":"@site/blog/2024-12-29-scd2.md","title":"Multitable SCD2 Joins - How to Unify Historical Changes","description":"In the realm of data management, historical changes are conventionally stored in separate Slowly Changing Dimension Type 2 (SCD2) tables. However, extracting point-in-time insights from these dispersed sources requires merging them into a single, unified entity.","date":"2024-12-29T00:00:00.000Z","tags":[{"inline":false,"label":"Data Model","permalink":"/portfolio-portal/blog/tags/data-model","description":"Data Model Disscussion"}],"readingTime":5.11,"hasTruncateMarker":true,"authors":[{"name":"Lewis Quoc Quang","title":"Solution Architect @ Rackspace","url":"https://www.linkedin.com/in/trinh-quoc-quang/","page":{"permalink":"/portfolio-portal/blog/authors/lewis"},"socials":{"github":"https://github.com/QuangTrinh1612","linkedin":"https://www.linkedin.com/in/trinh-quoc-quang/"},"imageURL":"https://avatars.githubusercontent.com/u/55908196?s=400&u=401c81ca1269100ff4dcf3ddcf11a1ff035cab42&v=4","key":"lewis"}],"frontMatter":{"slug":"multitable-scd2-joins","title":"Multitable SCD2 Joins - How to Unify Historical Changes","authors":["lewis"],"tags":["Data Model"]},"unlisted":false,"prevItem":{"title":"Power BI Data Model - Best Practice Analyzer (BPA)","permalink":"/portfolio-portal/blog/pbi-bpa-rules"}},"content":"In the realm of data management, historical changes are conventionally stored in separate Slowly Changing Dimension Type 2 (SCD2) tables. However, extracting point-in-time insights from these dispersed sources requires merging them into a single, unified entity.\\r\\n\\r\\nThis guide offers a succinct walkthrough of the process for performing multitable SCD2 joins, presenting two distinct approaches, Direct Join and Unified Timeline, evaluating their respective advantages and drawbacks through practical examples.\\r\\n\\r\\nLet\u2019s see how to effectively unify historical data and derive valuable insights\\r\\n\\r\\n\x3c!-- truncate --\x3e\\r\\n\\r\\n## Intent\\r\\nIn data warehousing, Slowly Changing Dimension Type 2 (SCD2) tables are widely used to track historical changes in dimension data. However, joining SCD2 tables correctly requires a specific approach to ensure accurate results. This article provides an understanding of the challenges involved and a step-by-step guide to implement robust joins for SCD2 tables.\\r\\n\\r\\n## Problem\\r\\nWhy is joining SCD2 tables challenging?\\r\\nSCD2 tables are designed to maintain historical data by creating new rows with updated attributes and validity periods. This introduces two main challenges:\\r\\n- Temporal Overlap: The same business key can exist in multiple rows, each representing a different validity period.\\r\\n- Point-in-Time Accuracy: Joining two SCD2 tables must align rows that were valid at the same point in time to ensure accurate results.\\r\\nA naive join may result in duplications or mismatched records due to overlapping validity periods.\\r\\n\\r\\n## Solution\\r\\n### Method 1: Direct Join with 02 SCD2 tables\\r\\n\\r\\n1. Perform this join condition that enables merging 2 rows that are in the same validity period\\r\\n\\r\\n```sql\\r\\nfrom scd2_table1 t1\\r\\njoin scd2_table2 t2\\r\\n  on t1.pk = t2.pk\\r\\n  and t1.valid_from < t2.valid_to \\r\\n  and t1.valid_to > t2.valid_from\\r\\n```\\r\\n\\r\\n2. Recalculate `valid_from` & `valid_to`\\r\\n- `valid_from` should be the latest one of the 2 `valid_from` of the 2 tables\\r\\n- `valid_to` should be the earliest one of the 2 `valid_to` of the 2 tables\\r\\n\\r\\n```sql\\r\\ngreatest(t1.valid_from, coalesce(t2.valid_from, \'1900-01-01\'::timestamp)) as valid_from,\\r\\ncoalesce(lead(greatest(t1.valid_from, coalesce(t2.valid_from, \'1900-01-01\'::timestamp)))\\r\\n    over (partition by t1.pk\\r\\n    order by greatest(t1.valid_from, coalesce(t2.valid_from, \'1900-01-01\'::timestamp))), \\r\\n          \'9999-12-31\'::timestamp) as valid_to\\r\\n```\\r\\n\\r\\n3. Filter out rows with `valid_from = valid_to` after join to avoid duplicated validity intervals\\r\\n\\r\\nIn most of real-life examples, `valid_from` and `valid_to` are usually in lower time granularity like timestamp (with the differences down to seconds), hence there might not be cases where these column value \u201coverlapped\u201d across multiple SCD2 tables and we might skip this step. However, for an overall solution, this should be a part of your joining condition.\\r\\n\\r\\n4. Combine them all together\\r\\n\\r\\n```sql\\r\\nwith\\r\\n    prep1 as (\\r\\n        select\\r\\n            t1.pk,\\r\\n            dim1,\\r\\n            dim2,\\r\\n            greatest(t1.valid_from, coalesce(t2.valid_from, \'1900-01-01\'::timestamp)) as valid_from,\\r\\n            coalesce(lead(greatest(t1.valid_from, coalesce(t2.valid_from, \'1900-01-01\'::timestamp)))\\r\\n                over (partition by t1.pk\\r\\n                order by greatest(t1.valid_from, coalesce(t2.valid_from, \'1900-01-01\'::timestamp))), \\r\\n                     \'9999-12-31\'::timestamp) as valid_to\\r\\n        from scd2_table1 t1\\r\\n        join scd2_table2 t2\\r\\n            on t1.pk = t2.pk\\r\\n            and t1.valid_from < t2.valid_to \\r\\n            and t1.valid_to > t2.valid_from\\r\\n    )\\r\\n\\r\\nselect \\r\\n    *\\r\\nfrom prep1\\r\\nwhere valid_from != valid_to\\r\\norder by PK, valid_from, valid_to, dim1, dim2\\r\\n```\\r\\n\\r\\n### Method 2: Unified timeline\\r\\nThis approach to multitable SCD2 joins first creates a unified timeline based on all the `valid_from` from referenced SCD2s. This timeline will be used as a scaffold for joining back all the SCD2s later on. Unlike Direct Join, the deduplication & `valid_to` recalculation steps are done during timeline unification. This allows us to execute all SCD2 joins in 1 CTE instead.\\r\\n\\r\\nNote that the process is quite similar when joining 02, 03 or more SCD2s:\\r\\n- Union PKs, `valid_from` from subsequent SCD2s;\\r\\n- Deduplicate the unioned table to make the following calculation lighter;\\r\\n- Recalculate `valid_to` from the unioned data to create the timeline;\\r\\n- Join subsequent SCD2s back to the timeline using this condition below. Repeat for all SCD2s.\\r\\n\\r\\n#### Unified timeline \u2013 Code Snippets\\r\\n\\r\\n```sql\\r\\nwith\\r\\n    unified_timeline as ( -- using union to deal with duplicates values instead of union all\\r\\n        select pk, valid_from from scd2_table1 union\\r\\n        select pk, valid_from from scd2_table2 union\\r\\n        select pk, valid_from from scd2_table3\\r\\n    ),\\r\\n    unified_timeline_recalculate_valid_to as (\\r\\n        select\\r\\n            pk,\\r\\n            valid_from,\\r\\n            coalesce(lead(valid_from) over(partition by pk order by valid_from), \'9999-12-31\'::timestamp) as valid_to,\\r\\n            valid_to = \'9999-12-31\'::timestamp as is_current\\r\\n        from unified_timeline\\r\\n    ),\\r\\n    joined as (\\r\\n        select\\r\\n            timeline.pk,\\r\\n            scd2_table1.dim1,\\r\\n            scd2_table2.dim2,\\r\\n            scd2_table3.dim3,\\r\\n            coalesce(timeline.valid_from, \'1900-01-01\'::timestamp) as valid_from,\\r\\n            coalesce(timeline.valid_to, \'9999-12-31\'::timestamp) as valid_to\\r\\n        from unified_timeline_recalculate_valid_to as timeline\\r\\n        left join scd2_table1\\r\\n            on timeline.pk = scd2_table1.pk \\r\\n            and scd2_table1.valid_from <= timeline.valid_from \\r\\n            and scd2_table1.valid_to >= timeline.valid_to\\r\\n        left join scd2_table2\\r\\n            on timeline.pk = scd2_table2.pk \\r\\n            and scd2_table2.valid_from <= timeline.valid_from \\r\\n            and scd2_table2.valid_to >= timeline.valid_to\\r\\n        left join scd2_table3\\r\\n            on timeline.pk = scd2_table1.pk \\r\\n            and scd2_table3.valid_from <= timeline.valid_from \\r\\n            and scd2_table3.valid_to >= timeline.valid_to\\r\\n    \\r\\n    )\\r\\nselect * from joined\\r\\n-- where valid_from != valid_to -- As we already have a distinct timeline (using union), this condition is no longer needed\\r\\norder by PK, valid_from, valid_to, dim1, dim2, dim3;\\r\\n```\\r\\n\\r\\n## Pros and Cons of Two Methods for Joining SCD2 Tables\\r\\n\\r\\n### Direct Joins\\r\\n| **Pros**                           | **Cons**                              |\\r\\n|------------------------------------|---------------------------------------|\\r\\n| Simple to implement and understand.| Can result in incorrect joins if overlapping validity periods exist. |\\r\\n| Efficient for small datasets.      | Requires careful handling of `valid_to` (e.g., NULL values).          |\\r\\n| No need for pre-processing.        | May fail to align records accurately in complex scenarios.            |\\r\\n\\r\\n### Unified Timeline Method\\r\\n| **Pros**                           | **Cons**                              |\\r\\n|------------------------------------|---------------------------------------|\\r\\n| Ensures accurate alignment of records across tables. | More complex to implement and requires additional processing. |\\r\\n| Handles overlapping validity periods effectively. | May require significant compute resources for large datasets. |\\r\\n| Suitable for point-in-time analysis and historical consistency. | Slightly slower due to timeline unification steps.           |\\r\\n\\r\\nBy following these steps, you can reliably join SCD2 tables while maintaining historical and point-in-time accuracy. Proper implementation ensures accurate data analysis and minimizes the risk of logical errors in your data pipeline.\\r\\n\\r\\n## Summary\\r\\n- **Problem**: Joining SCD2 tables can be challenging due to overlapping validity periods and the need for point-in-time accuracy.\\r\\n- **Solution**: To address this, filter records using the valid_from and valid_to columns and perform a temporal join based on both business keys and validity periods.\\r\\n- **Implementation**: The solution is implemented using SQL, with a focus on defining a point-in-time context, filtering valid records, and aligning temporal validity.\\r\\n- **Comparison of Methods**:\\r\\n    - Direct Joins: Simple to implement but prone to errors with overlapping periods.\\r\\n    - Unified Timeline Method: Ensures accuracy but is more complex and resource-intensive."}]}}')}}]);